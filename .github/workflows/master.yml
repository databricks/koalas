name: master

on:
  push:
    branches: 
      - master
  pull_request:
    branches: 
      - master

jobs:
  python35:
    runs-on: ubuntu-latest
    env:
      SPARK_VERSION: 2.3.4
      PANDAS_VERSION: 0.23.4
      PYARROW_VERSION: 0.10.0
      PYTHON_EXECUTABLE: xvfb-run python
    steps:
    - uses: actions/checkout@v2
    - uses: actions/setup-java@v1
      with:
        java-version: 1.8
    - name: Using cache
      uses: actions/cache@v1
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-dev.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    # Setup Python3.5 via `apt-get install` since the dafault Python3.5 from `actions/setup-python`
    # seems to have some problem with Tkinter, so we should manually install the python3.5-tk.
    # for this, we should use the Python manually installed, not the default one from `actions/setup-python`
    - name: Setup Python 3.5
      run: |
        sudo add-apt-repository ppa:deadsnakes/ppa
        sudo apt-get install tk-dev python3.5-tk python3.5
        sudo rm /usr/bin/python
        sudo ln -s /usr/bin/python3.5 /usr/bin/python
        # Below command is required for ensuring pip installed executables to be in the path.
        echo "::add-path::/home/runner/.local/bin"
    - name: Install dependencies & run tests
      run: |
        ./dev/download_travis_dependencies.sh
        sudo apt-get install xclip
        pip install setuptools
        pip install -r requirements-dev.txt
        pip install pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION
        pip list
        export SPARK_HOME="$HOME/.cache/spark-versions/spark-$SPARK_VERSION-bin-hadoop2.7"
        ./dev/lint-python
        ./dev/pytest
    # Push the results back to codecov
    - name: Update Codecov
      run: bash <(curl -s https://codecov.io/bash)

  python:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.6, 3.7]
        include:
          - python-version: 3.6
            spark-version: 2.4.4
            pandas-version: 0.24.2
            pyarrow-version: 0.13.0
            logger: databricks.koalas.usage_logging.usage_logger
          - python-version: 3.7
            spark-version: 2.4.4
            pandas-version: 0.25.3
            pyarrow-version: 0.14.1
    env:
      PYTHON_VERSION: ${{ matrix.python-version }}
      SPARK_VERSION: ${{ matrix.spark-version }}
      PANDAS_VERSION: ${{ matrix.pandas-version }}
      PYARROW_VERSION: ${{ matrix.pyarrow-version }}
      DISPLAY: 0.0
      QT_QPA_PLATFORM: offscreen
      KOALAS_USAGE_LOGGER: ${{ matrix.logger }}
    steps:
    - uses: actions/checkout@v2
    - uses: actions/setup-java@v1
      with:
        java-version: 1.8
    - name: Using cache
      uses: actions/cache@v1
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements-dev.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    - name: Install dependencies & run tests
      run: |
        ./dev/download_travis_dependencies.sh
        curl -s https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda.sh
        bash miniconda.sh -b -p $HOME/miniconda
        . $HOME/miniconda/etc/profile.d/conda.sh
        hash -r
        conda config --set always_yes yes --set changeps1 no
        conda update -q conda
        # Useful for debugging any issues with conda
        conda info -a
        # Replace dep1 dep2 ... with your dependencies
        conda create -c conda-forge -q -n test-environment python=$PYTHON_VERSION
        conda activate test-environment
        conda install -c conda-forge --yes codecov
        conda config --env --add pinned_packages python=$PYTHON_VERSION
        conda config --env --add pinned_packages pandas==$PANDAS_VERSION
        conda config --env --add pinned_packages pyarrow==$PYARROW_VERSION
        conda install -c conda-forge --yes pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION
        conda install -c conda-forge --yes --freeze-installed --file requirements-dev.txt
        conda list
        export SPARK_HOME="$HOME/.cache/spark-versions/spark-$SPARK_VERSION-bin-hadoop2.7"
        ./dev/lint-python
        ./dev/pytest
    # Push the results back to codecov
    - name: Update Codecov
      run: bash <(curl -s https://codecov.io/bash)
