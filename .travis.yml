language: python

cache:
  directories:
    - $HOME/.cache/spark-versions

matrix:
  include:
#    - python: "2.7"
#      env:
#        - SPARK_VERSION=2.3.2
#        - PANDAS_VERSION=0.19.2
#        - PYARROW_VERSION=0.8.0
    - python: "3.5"
      env:
        - SPARK_VERSION=2.3.3
        - PANDAS_VERSION=0.19.2
        - PYARROW_VERSION=0.10.0
    - python: "3.6"
      env:
        - SPARK_VERSION=2.4.1
        - PANDAS_VERSION=0.23.4
        - PYARROW_VERSION=0.10.0

before_install:
 - ./bin/download_travis_dependencies.sh

# See this page: http://conda.pydata.org/docs/travis.html
install:
  - sudo apt-get update
  # We do this conditionally because it saves us some downloading if the
  # version is the same.
  - if [[ "$TRAVIS_PYTHON_VERSION" == "2.7" ]]; then
      curl -s https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh > miniconda.sh;
    else
      curl -s https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda.sh;
    fi
  - bash miniconda.sh -b -p $HOME/miniconda
  - export PATH="$HOME/miniconda/bin:$PATH"
  - hash -r
  - conda config --set always_yes yes --set changeps1 no
  - conda update -q conda
  # Useful for debugging any issues with conda
  - conda info -a

  # Replace dep1 dep2 ... with your dependencies
  - conda create -c conda-forge -q -n test-environment python=$TRAVIS_PYTHON_VERSION nose==1.3.7 flake8>=3.5.0 pandas==$PANDAS_VERSION pyarrow==$PYARROW_VERSION decorator
  - source activate test-environment

  - conda list

before_script:
  - ./dev/lint-python

script:
  - SPARK_HOME="$HOME/.cache/spark-versions/spark-$SPARK_VERSION-bin-hadoop2.7" ./dev/run-tests.sh
